{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d40810e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. ÏÑ§Ï†ï (Configuration) ---\n",
    "CONFIG = {\n",
    "    \"image_dir\": \"sleepy/dataset/Trainning/image_trainning/all_image_trainning\",\n",
    "    \"label_dir\": \"sleepy/dataset/Trainning/label_trainning/all_label_trainning\",\n",
    "    \"test_image_dir\": \"sleepy/dataset/Test/image_test/all_image_test\",\n",
    "    \"test_label_dir\": \"sleepy/dataset/Test/label_test/all_label_test\",\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"img_size\": 90,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 5,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"max_samples\": 9000,\n",
    "    \"model_path\": \"eye_state_classifier.pth\"\n",
    "}\n",
    "\n",
    "# --- 2. Ïú†Ìã∏Î¶¨Ìã∞ Ìï®Ïàò ---\n",
    "def str2bool(val: str) -> bool:\n",
    "    return str(val).lower() in (\"true\", \"1\")\n",
    "\n",
    "def crop_and_resize_eye(frame: np.ndarray, bbox: tuple, size: int = 90) -> np.ndarray | None:\n",
    "    if bbox is None:\n",
    "        return None\n",
    "    x, y, w, h = bbox\n",
    "    x1 = max(x - int(w * 0.5), 0)\n",
    "    y1 = max(y - int(h * 0.5), 0)\n",
    "    x2 = min(x + w + int(w * 0.5), frame.shape[1])\n",
    "    y2 = min(y + h + int(h * 0.5), frame.shape[0])\n",
    "    eye_crop = frame[y1:y2, x1:x2]\n",
    "    if eye_crop.size == 0:\n",
    "        return None\n",
    "    return cv2.resize(eye_crop, (size, size))\n",
    "\n",
    "# --- 3. Dataset ÌÅ¥ÎûòÏä§ ---\n",
    "class EyeDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None, max_samples=None):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.label_dir = Path(label_dir)\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        image_files = sorted([f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        if max_samples:\n",
    "            image_files = image_files[:max_samples]\n",
    "\n",
    "        print(\"üîç Ïú†Ìö®Ìïú Îàà Îç∞Ïù¥ÌÑ∞Î•º ÏàòÏßë Ï§ë...\")\n",
    "        for img_name in tqdm(image_files):\n",
    "            img_path = self.image_dir / img_name\n",
    "            label_path = self.label_dir / (img_name.rsplit('.', 1)[0] + '.json')\n",
    "            if not label_path.exists():\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with open(label_path, 'r', encoding='utf-8') as f:\n",
    "                    label_data = json.load(f)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            object_info = label_data.get('ObjectInfo', {})\n",
    "            keypoints_info = object_info.get('KeyPoints', {})\n",
    "            points = keypoints_info.get('Points')\n",
    "            bounding_boxes = object_info.get('BoundingBox', {})\n",
    "\n",
    "            if not points or len(points) < (47 * 2):\n",
    "                continue\n",
    "            try:\n",
    "                points_arr = np.array(points, dtype=np.float32).reshape(-1, 2)\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "\n",
    "            leye_indices = [36, 37, 38, 39, 40, 41]\n",
    "            reye_indices = [42, 43, 44, 45, 46, 47]\n",
    "\n",
    "            leye_points = points_arr[leye_indices]\n",
    "            reye_points = points_arr[reye_indices]\n",
    "\n",
    "            lx_min, ly_min = np.min(leye_points, axis=0)\n",
    "            lx_max, ly_max = np.max(leye_points, axis=0)\n",
    "            rx_min, ry_min = np.min(reye_points, axis=0)\n",
    "            rx_max, ry_max = np.max(reye_points, axis=0)\n",
    "\n",
    "            leye_bbox = (int(lx_min), int(ly_min), int(lx_max - lx_min), int(ly_max - ly_min))\n",
    "            reye_bbox = (int(rx_min), int(ry_min), int(rx_max - rx_min), int(ry_max - ry_min))\n",
    "\n",
    "            leye_opened = str2bool(bounding_boxes.get('Leye', {}).get('Opened', False))\n",
    "            reye_opened = str2bool(bounding_boxes.get('Reye', {}).get('Opened', False))\n",
    "\n",
    "            if leye_bbox[2] > 0 and leye_bbox[3] > 0:\n",
    "                self.samples.append((str(img_path), leye_bbox, int(leye_opened)))\n",
    "            if reye_bbox[2] > 0 and reye_bbox[3] > 0:\n",
    "                self.samples.append((str(img_path), reye_bbox, int(reye_opened)))\n",
    "\n",
    "        print(f\"‚úÖ Ï¥ù {len(self.samples)}Í∞úÏùò Îàà Ïù¥ÎØ∏ÏßÄ ÏàòÏßë ÏôÑÎ£å\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, bbox, label = self.samples[idx]\n",
    "        image_cv = cv2.imread(img_path)\n",
    "        if image_cv is None:\n",
    "            return None, None\n",
    "        eye_img = crop_and_resize_eye(image_cv, bbox, size=CONFIG['img_size'])\n",
    "        if eye_img is None:\n",
    "            return None, None\n",
    "        image_pil = Image.fromarray(cv2.cvtColor(eye_img, cv2.COLOR_BGR2RGB))\n",
    "        if self.transform:\n",
    "            image = self.transform(image_pil)\n",
    "        return image, torch.tensor(label, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df215952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Ïú†Ìö®Ìïú Îàà Îç∞Ïù¥ÌÑ∞Î•º ÏàòÏßë Ï§ë...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9000/9000 [00:03<00:00, 2420.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ï¥ù 17990Í∞úÏùò Îàà Ïù¥ÎØ∏ÏßÄ ÏàòÏßë ÏôÑÎ£å\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17990"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = EyeDataset(CONFIG['image_dir'], CONFIG['label_dir'], transform=transform, max_samples=CONFIG['max_samples'])\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7484088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e991054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Î™®Îç∏ Ï†ïÏùò ---\n",
    "class EyeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),  # 90x90 -> 90x90\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                 # 90x90 -> 45x45\n",
    "\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                 # 45x45 -> 22x22\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                 # 22x22 -> 11x11\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),   \n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 5 * 5, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),  # 2 classes: open/closed\n",
    "            nn.Sigmoid()  # Output between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33c0ba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. ÌïôÏäµ Î∞è ÌèâÍ∞Ä ---\n",
    "def train_model(model, train_loader, criterion, optimizer, device, epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    print(\"\\n=== üöÄ ÌïôÏäµ ÏãúÏûë ===\")\n",
    "    for epoch in range(5):\n",
    "        total_loss = 0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/1\"):\n",
    "            if images is None: continue\n",
    "            images = images.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}\")\n",
    "        losses.append(avg_loss)\n",
    "    return losses\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    print(\"\\n=== üìä ÌÖåÏä§Ìä∏ ÌèâÍ∞Ä Ï§ë ===\")\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader):\n",
    "            if images is None: continue\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            predicted = (outputs > 0.5).int().cpu().squeeze()\n",
    "            preds.append(predicted)\n",
    "            trues.append(labels)\n",
    "\n",
    "    y_pred = torch.cat([p.view(-1) for p in preds]).numpy()\n",
    "    y_true = torch.cat([l.view(-1) for l in trues]).numpy()\n",
    "\n",
    "    print(\"\\n[ÌèâÍ∞Ä Í≤∞Í≥º]\")\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"Closed (0)\", \"Opened (1)\"], zero_division=0))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(f\"\\nAccuracy: {accuracy_score(y_true, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42a88ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª ÎîîÎ∞îÏù¥Ïä§: cuda\n",
      "üîç Ïú†Ìö®Ìïú Îàà Îç∞Ïù¥ÌÑ∞Î•º ÏàòÏßë Ï§ë...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9000/9000 [00:03<00:00, 2672.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ï¥ù 17990Í∞úÏùò Îàà Ïù¥ÎØ∏ÏßÄ ÏàòÏßë ÏôÑÎ£å\n",
      "üîç Ïú†Ìö®Ìïú Îàà Îç∞Ïù¥ÌÑ∞Î•º ÏàòÏßë Ï§ë...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 2824.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ï¥ù 1000Í∞úÏùò Îàà Ïù¥ÎØ∏ÏßÄ ÏàòÏßë ÏôÑÎ£å\n",
      "\n",
      "ÌïôÏäµ ÏÉòÌîå Ïàò: 17990\n",
      "ÌÖåÏä§Ìä∏ ÏÉòÌîå Ïàò: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Î©îÏù∏ Î∏îÎ°ù ---\n",
    "\n",
    "print(f\"üíª ÎîîÎ∞îÏù¥Ïä§: {CONFIG['device']}\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,)*3, (0.5,)*3)\n",
    "])\n",
    "\n",
    "# ÌïôÏäµ/ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ÏÖã Ï§ÄÎπÑ\n",
    "train_dataset = EyeDataset(CONFIG['image_dir'], CONFIG['label_dir'], transform=transform, max_samples=CONFIG['max_samples'])\n",
    "test_dataset = EyeDataset(CONFIG['test_image_dir'], CONFIG['test_label_dir'], transform=transform)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x[0] is not None, batch))\n",
    "    if not batch:\n",
    "        return torch.Tensor(), torch.Tensor()\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"\\nÌïôÏäµ ÏÉòÌîå Ïàò: {len(train_dataset)}\")\n",
    "print(f\"ÌÖåÏä§Ìä∏ ÏÉòÌîå Ïàò: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ee410c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Î™®Îç∏ Ï†ïÏùò Î∞è ÌïôÏäµ\n",
    "model = EyeCNN().to(CONFIG['device'])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ff26fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== üöÄ ÌïôÏäµ ÏãúÏûë ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 563/563 [02:04<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1:  21%|‚ñà‚ñà‚ñè       | 120/563 [00:25<01:34,  4.71it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_model(model, train_loader, criterion, optimizer, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[29], line 8\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, criterion, optimizer, device, epochs)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      7\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/1\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\JH\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JH\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\JH\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\JH\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[31], line 118\u001b[0m, in \u001b[0;36mEyeDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image_cv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m eye_img \u001b[38;5;241m=\u001b[39m crop_and_resize_eye(image_cv, bbox, size\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_size\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eye_img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[31], line 47\u001b[0m, in \u001b[0;36mcrop_and_resize_eye\u001b[1;34m(frame, bbox, size)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eye_crop\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mresize(eye_crop, (size, size))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, criterion, optimizer, device='cuda:0', epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2af06459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:05<00:00,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Closed (0)       1.00      1.00      1.00      1000\n",
      "  Opened (1)       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           1.00      1000\n",
      "   macro avg       0.50      0.50      0.50      1000\n",
      "weighted avg       1.00      1.00      1.00      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ÌèâÍ∞Ä Ìï®ÏàòÏóêÏÑú Í≤∞Í≥º Î∞òÌôòÌïòÎèÑÎ°ù ÏàòÏ†ï\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader):\n",
    "            if images is None: continue\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            predicted = (outputs > 0.5).int().cpu().squeeze()\n",
    "            preds.append(predicted)\n",
    "            trues.append(labels)\n",
    "    y_pred = torch.cat([p.view(-1) for p in preds]).numpy()\n",
    "    y_true = torch.cat([l.view(-1) for l in trues]).numpy()\n",
    "    return y_true, y_pred\n",
    "\n",
    "# Î©îÏù∏ÏóêÏÑú ÌèâÍ∞Ä Í≤∞Í≥º Ï∂úÎ†•\n",
    "y_true, y_pred = evaluate_model(model, test_loader, CONFIG['device'])\n",
    "print(classification_report(y_true, y_pred, target_names=['Closed (0)', 'Opened (1)'], labels=[0, 1], zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89450124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JH\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1000]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92e70af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63692e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(losses, label='Train Loss')\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
