{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d7876e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c33e21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"image_dir\": \"sleepy/dataset/Trainning/image_trainning/all_image_trainning\",\n",
    "    \"label_dir\": \"sleepy/dataset/Trainning/label_trainning/all_label_trainning\",\n",
    "    \"test_image_dir\": \"sleepy/dataset/Test/image_test/all_image_test\",\n",
    "    \"test_label_dir\": \"sleepy/dataset/Test/label_test/all_label_test\",\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"img_size\": 90,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 5,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"max_samples\": 15000,\n",
    "    \"model_path\": \"eye_state_classifier.pth\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53ce4aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(val: str) -> bool:\n",
    "    return str(val).strip().lower() == \"true\"\n",
    "\n",
    "def crop_and_resize_eye(frame: np.ndarray, bbox: tuple, size: int = 90) -> np.ndarray | None:\n",
    "    if bbox is None:\n",
    "        return None\n",
    "    x, y, w, h = bbox\n",
    "    x1 = max(x - int(w * 0.5), 0)\n",
    "    y1 = max(y - int(h * 0.5), 0)\n",
    "    x2 = min(x + w + int(w * 0.5), frame.shape[1])\n",
    "    y2 = min(y + h + int(h * 0.5), frame.shape[0])\n",
    "    eye_crop = frame[y1:y2, x1:x2]\n",
    "    if eye_crop.size == 0:\n",
    "        return None\n",
    "    return cv2.resize(eye_crop, (size, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06867e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EyeDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None, max_samples=None):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.label_dir = Path(label_dir)\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        image_files = sorted([f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        if max_samples:\n",
    "            image_files = image_files[:max_samples]\n",
    "\n",
    "        print(\"ğŸ” ìœ íš¨í•œ ëˆˆ ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ì¤‘...\")\n",
    "        for img_name in tqdm(image_files):\n",
    "            img_path = self.image_dir / img_name\n",
    "            label_path = self.label_dir / (img_name.rsplit('.', 1)[0] + '.json')\n",
    "            if not label_path.exists():\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with open(label_path, 'r', encoding='utf-8') as f:\n",
    "                    label_data = json.load(f)\n",
    "            except Exception:\n",
    "                continue\n",
    "            \n",
    "            object_info = label_data.get('ObjectInfo', {})\n",
    "            bounding_boxes = object_info.get('BoundingBox', {})\n",
    "            \n",
    "            leye_info = bounding_boxes.get('Leye', {})\n",
    "            reye_info = bounding_boxes.get('Reye', {})\n",
    "\n",
    "            if not (leye_info.get('isVisible', False) and reye_info.get('isVisible', False)):\n",
    "                continue\n",
    "\n",
    "            leye_opened = str2bool(leye_info.get('Opened', 'false'))\n",
    "            reye_opened = str2bool(reye_info.get('Opened', 'false'))\n",
    "\n",
    "            leye_pos = leye_info.get('Position', [0, 0, 0, 0])\n",
    "            reye_pos = reye_info.get('Position', [0, 0, 0, 0])\n",
    "            \n",
    "            try:\n",
    "                leye_bbox = tuple(map(int, map(float, leye_pos)))\n",
    "                reye_bbox = tuple(map(int, map(float, reye_pos)))\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "\n",
    "            if leye_bbox[2] > 0 and leye_bbox[3] > 0:\n",
    "                self.samples.append((str(img_path), leye_bbox, int(leye_opened)))\n",
    "            if reye_bbox[2] > 0 and reye_bbox[3] > 0:\n",
    "                self.samples.append((str(img_path), reye_bbox, int(reye_opened)))\n",
    "            \n",
    "        print(f\"âœ… ì´ {len(self.samples)}ê°œì˜ ëˆˆ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì™„ë£Œ\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, bbox, label = self.samples[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ê²½ê³ : íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {img_path}\")\n",
    "            return None, None\n",
    "\n",
    "        x, y, w, h = bbox\n",
    "        if w <= 0 or h <= 0:\n",
    "            return None, None\n",
    "        \n",
    "        eye_img = image.crop((x, y, x + w, y + h))\n",
    "\n",
    "        if self.transform:\n",
    "            eye_img = self.transform(eye_img)\n",
    "\n",
    "        return eye_img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7879727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EyeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 5 * 5, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        # Flatten í›„ FC ë ˆì´ì–´ì— ë§ëŠ” í¬ê¸°ë¡œ reshape\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # EyeCNN ëª¨ë¸ì˜ fc ë ˆì´ì–´ í¬ê¸°ê°€ 128 * 5 * 5ë¡œ í•˜ë“œì½”ë”© ë˜ì–´ìˆì–´\n",
    "        # ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°ê°€ 90ì´ ì•„ë‹ ê²½ìš° ì—ëŸ¬ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "        # MaxPool2dë¥¼ 4ë²ˆ ê±°ì¹œ í›„ í¬ê¸°ëŠ” 90 -> 45 -> 22 -> 11 -> 5 ê°€ ë©ë‹ˆë‹¤.\n",
    "        # ë”°ë¼ì„œ Flatten í›„ 128 * 5 * 5ê°€ ë©ë‹ˆë‹¤.\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4379f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ìœ íš¨í•œ ëˆˆ ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13446/13446 [00:01<00:00, 7139.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì´ 6960ê°œì˜ ëˆˆ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì™„ë£Œ\n",
      "ğŸ” ìœ íš¨í•œ ëˆˆ ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 510/510 [00:00<00:00, 5994.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì´ 20ê°œì˜ ëˆˆ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì™„ë£Œ\n",
      "\n",
      "í•™ìŠµ ìƒ˜í”Œ ìˆ˜: 6960\n",
      "í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: 20\n",
      "\n",
      "=== ğŸš€ í•™ìŠµ ì‹œì‘ ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 218/218 [00:58<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Loss: 0.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 140/218 [00:39<00:23,  3.37it/s]"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, device, epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    print(\"\\n=== ğŸš€ í•™ìŠµ ì‹œì‘ ===\")\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            if images.nelement() == 0:\n",
    "                continue\n",
    "            images = images.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if len(train_loader) > 0:\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f}\")\n",
    "            losses.append(avg_loss)\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | í•™ìŠµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    return losses\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    print(\"\\n=== ğŸ“Š í…ŒìŠ¤íŠ¸ í‰ê°€ ì‹œì‘ ===\")\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"í‰ê°€ ì¤‘\"):\n",
    "            if images.nelement() == 0:\n",
    "                continue\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            predicted = (outputs > 0.5).int().cpu()\n",
    "            preds.append(predicted)\n",
    "            trues.append(labels.cpu())\n",
    "\n",
    "    if not preds:\n",
    "        print(\"â— ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    y_pred = torch.cat(preds).view(-1).numpy()\n",
    "    y_true = torch.cat(trues).view(-1).numpy()\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c36cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # ë°ì´í„° ë³€í™˜ ì •ì˜\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ì¤€ë¹„\n",
    "    train_dataset = EyeDataset(CONFIG['image_dir'], CONFIG['label_dir'], transform=transform, max_samples=CONFIG['max_samples'])\n",
    "    test_dataset = EyeDataset(CONFIG['test_image_dir'], CONFIG['test_label_dir'], transform=transform)\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        batch = list(filter(lambda x: x[0] is not None, batch))\n",
    "        if not batch:\n",
    "            return torch.Tensor(), torch.Tensor()\n",
    "        return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    print(f\"\\ní•™ìŠµ ìƒ˜í”Œ ìˆ˜: {len(train_dataset)}\")\n",
    "    print(f\"í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(test_dataset)}\")\n",
    "\n",
    "    # ëª¨ë¸, ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì •ì˜\n",
    "    model = EyeCNN().to(CONFIG['device'])\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "\n",
    "    # ëª¨ë¸ í•™ìŠµ\n",
    "    losses = train_model(model, train_loader, criterion, optimizer, CONFIG['device'], CONFIG['epochs'])\n",
    "\n",
    "    # í•™ìŠµëœ ëª¨ë¸ ì €ì¥\n",
    "    torch.save(model.state_dict(), CONFIG['model_path'])\n",
    "    print(f\"\\nâœ… ëª¨ë¸ì´ '{CONFIG['model_path']}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    # ëª¨ë¸ í‰ê°€\n",
    "    # (ì„ íƒ ì‚¬í•­) ì €ì¥ëœ ëª¨ë¸ì„ ë‹¤ì‹œ ë¡œë“œí•˜ì—¬ í‰ê°€í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "    # model.load_state_dict(torch.load(CONFIG['model_path']))\n",
    "    y_true, y_pred = evaluate_model(model, test_loader, CONFIG['device'])\n",
    "    \n",
    "    # í‰ê°€ ê²°ê³¼ ì¶œë ¥\n",
    "    if len(y_true) > 0:\n",
    "        print(\"\\n[ ìµœì¢… í‰ê°€ ê²°ê³¼ ]\")\n",
    "        print(classification_report(y_true, y_pred, target_names=[\"Closed (0)\", \"Opened (1)\"], zero_division=0))\n",
    "        \n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(y_true, y_pred))\n",
    "        \n",
    "        print(f\"\\nAccuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    \n",
    "    # ì†ì‹¤ ê·¸ë˜í”„ ì‹œê°í™”\n",
    "    if losses:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(losses, label='Train Loss')\n",
    "        plt.title(\"Training Loss per Epoch\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
