{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "764639db-0fab-459f-af83-4e00ca318f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\anaconda3\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in c:\\anaconda3\\lib\\site-packages (0.22.1)\n",
      "Requirement already satisfied: torchaudio in c:\\anaconda3\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: lightning in c:\\anaconda3\\lib\\site-packages (2.5.2)\n",
      "Requirement already satisfied: filelock in c:\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\anaconda3\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda3\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in c:\\anaconda3\\lib\\site-packages (from lightning) (6.0.1)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in c:\\anaconda3\\lib\\site-packages (from lightning) (0.14.3)\n",
      "Requirement already satisfied: packaging<27.0,>=20.0 in c:\\anaconda3\\lib\\site-packages (from lightning) (24.1)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in c:\\anaconda3\\lib\\site-packages (from lightning) (1.7.4)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in c:\\anaconda3\\lib\\site-packages (from lightning) (4.66.5)\n",
      "Requirement already satisfied: pytorch-lightning in c:\\anaconda3\\lib\\site-packages (from lightning) (2.5.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\anaconda3\\lib\\site-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (3.10.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\anaconda3\\lib\\site-packages (from tqdm<6.0,>=4.57.0->lightning) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.11.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (3.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a05259db-19e6-4b4c-9676-73361df2a832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "791b0cba-7f6e-4db1-ae31-4cdb23f93dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›¹ìº  ì‹¤í–‰ ì¤‘... 's' í‚¤ë¥¼ ëˆ„ë¥´ë©´ ëˆˆì„ ì €ì¥í•˜ê³ , 'q' í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.\n",
      "ì¢…ë£Œí•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "SAVE_DIR = r\"C:\\Users\\Users\\eye_captures\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# Mediapipe ì´ˆê¸° ì„¤ì •\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    refine_landmarks=True,\n",
    "    max_num_faces=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ëˆˆ ëœë“œë§ˆí¬ ì¸ë±ìŠ¤ (ì •í™•í•œ ì¢Œìš° ê¸°ì¤€)\n",
    "RIGHT_EYE_IDX = [33, 133, 160, 159, 158, 157, 173, 246]   # ì‚¬ëŒì˜ ì˜¤ë¥¸ìª½ ëˆˆ (í™”ë©´ì—ì„œëŠ” ì™¼ìª½)\n",
    "LEFT_EYE_IDX = [362, 263, 387, 386, 385, 384, 398, 466]   # ì‚¬ëŒì˜ ì™¼ìª½ ëˆˆ (í™”ë©´ì—ì„œëŠ” ì˜¤ë¥¸ìª½)\n",
    "\n",
    "\n",
    "\n",
    "# ëˆˆ ì´ë¯¸ì§€ ì¶”ì¶œ í•¨ìˆ˜\n",
    "def extract_eye(image, landmarks, eye_indices, padding=20):\n",
    "    ih, iw, _ = image.shape\n",
    "    eye_points = np.array([(int(landmarks[i].x * iw), int(landmarks[i].y * ih)) for i in eye_indices])\n",
    "    x, y, w, h = cv2.boundingRect(eye_points)\n",
    "\n",
    "    x = max(x - padding, 0)\n",
    "    y = max(y - padding, 0)\n",
    "    w = min(w + padding * 2, iw - x)\n",
    "    h = min(h + padding * 2, ih - y)\n",
    "\n",
    "    eye_img = image[y:y+h, x:x+w]\n",
    "    return eye_img\n",
    "\n",
    "# ì›¹ìº  ì‹œì‘\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"ì›¹ìº  ì‹¤í–‰ ì¤‘... 's' í‚¤ë¥¼ ëˆ„ë¥´ë©´ ëˆˆì„ ì €ì¥í•˜ê³ , 'q' í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.\")\n",
    "\n",
    "left_eye_img = None\n",
    "right_eye_img = None\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print(\"ì¹´ë©”ë¼ í”„ë ˆì„ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        break\n",
    "\n",
    "    # RGB ë³€í™˜\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "\n",
    "    # ëˆˆ ì˜ì—­ ì¶”ì¶œ\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            landmarks = face_landmarks.landmark\n",
    "            try:\n",
    "                left_eye_img = extract_eye(frame, landmarks, LEFT_EYE_IDX)\n",
    "                right_eye_img = extract_eye(frame, landmarks, RIGHT_EYE_IDX)\n",
    "\n",
    "                # ëˆˆ ì´ë¯¸ì§€ ì‹¤ì‹œê°„ ì¶œë ¥\n",
    "                if left_eye_img.size > 0:\n",
    "                    cv2.imshow(\"Left Eye\", left_eye_img)\n",
    "                if right_eye_img.size > 0:\n",
    "                    cv2.imshow(\"Right Eye\", right_eye_img)\n",
    "            except Exception as e:\n",
    "                print(\"ëˆˆ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:\", e)\n",
    "                continue\n",
    "\n",
    "    # ì›ë³¸ ì˜ìƒ í‘œì‹œ\n",
    "    cv2.imshow(\"Webcam\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # ëˆˆ ì €ì¥: 's' í‚¤\n",
    "    if key == ord('s') and left_eye_img is not None and right_eye_img is not None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S%f\")\n",
    "        left_path = f\"left_eye_{timestamp}.png\"\n",
    "        right_path = f\"right_eye_{timestamp}.png\"\n",
    "\n",
    "        # âœ… ëˆˆ ì´ë¯¸ì§€ 90x90ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆ\n",
    "        left_resized = cv2.resize(left_eye_img, (90, 90))\n",
    "        right_resized = cv2.resize(right_eye_img, (90, 90))\n",
    "\n",
    "        # âœ… ì €ì¥\n",
    "        cv2.imwrite(os.path.join(SAVE_DIR, left_path), left_resized)\n",
    "        cv2.imwrite(os.path.join(SAVE_DIR, right_path), right_resized)\n",
    "\n",
    "        print(f\"ëˆˆ ì €ì¥ ì™„ë£Œ: {left_path}, {right_path}\")\n",
    "\n",
    "    # ì¢…ë£Œ: 'q' í‚¤\n",
    "    elif key == ord('q'):\n",
    "        print(\"ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "        break\n",
    "\n",
    "# ì¢…ë£Œ ì²˜ë¦¬\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "583f6bc3-1751-406e-8055-9117e0e3d14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: C:\\Windows\\System32\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46a17e1f-5742-4d15-9433-aba30c9ff1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train/open] ì´ë¯¸ì§€ 1000ì¥ ì²˜ë¦¬ ì¤‘...\n",
      "[train/closed] ì´ë¯¸ì§€ 1000ì¥ ì²˜ë¦¬ ì¤‘...\n",
      "[val/open] ì´ë¯¸ì§€ 1000ì¥ ì²˜ë¦¬ ì¤‘...\n",
      "[val/closed] ì´ë¯¸ì§€ 1000ì¥ ì²˜ë¦¬ ì¤‘...\n",
      "[test/open] ì´ë¯¸ì§€ 1000ì¥ ì²˜ë¦¬ ì¤‘...\n",
      "[test/closed] ì´ë¯¸ì§€ 1000ì¥ ì²˜ë¦¬ ì¤‘...\n",
      "\n",
      "ğŸ“Š ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½\n",
      "ì´ ì´ë¯¸ì§€ ìˆ˜     : 6000\n",
      "ì •í™•íˆ ì˜ˆì¸¡í•œ ìˆ˜ : 3364\n",
      "ì •í™•ë„           : 56.07%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Mediapipe ì„¤ì •\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=True,\n",
    "    refine_landmarks=True,\n",
    "    max_num_faces=1,\n",
    "    min_detection_confidence=0.5\n",
    ")\n",
    "\n",
    "# ëˆˆ ê°ê¹€ íŒë³„ í•¨ìˆ˜\n",
    "def is_eye_closed(upper_y, lower_y, threshold=3):\n",
    "    return abs(upper_y - lower_y) < threshold\n",
    "\n",
    "# ê¸°ì¤€ ê²½ë¡œ\n",
    "BASE_PATH = r\"C:\\Users\\Users\\open-closed-eyes-dataset\"\n",
    "sets = ['train', 'val', 'test']\n",
    "labels = ['open', 'closed']\n",
    "MAX_IMAGES = 1000  # í´ë”ë‹¹ ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "results = []\n",
    "\n",
    "for subset in sets:\n",
    "    for label in labels:\n",
    "        folder_path = os.path.join(BASE_PATH, subset, label)\n",
    "        image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        image_files = random.sample(image_files, min(MAX_IMAGES, len(image_files)))  # ëœë¤ ì„ íƒ\n",
    "\n",
    "        print(f\"[{subset}/{label}] ì´ë¯¸ì§€ {len(image_files)}ì¥ ì²˜ë¦¬ ì¤‘...\")\n",
    "\n",
    "        for filename in image_files:\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            image = cv2.imread(file_path)\n",
    "\n",
    "            if image is None:\n",
    "                print(f\"  - ì´ë¯¸ì§€ ë¡œë”© ì‹¤íŒ¨: {filename}\")\n",
    "                continue\n",
    "\n",
    "            ih, iw, _ = image.shape\n",
    "            rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results_mp = face_mesh.process(rgb)\n",
    "\n",
    "            # ê¸°ë³¸ íŒì •: ëˆˆì„ ê°ì€ ìƒíƒœ\n",
    "            pred_label = \"closed\"\n",
    "\n",
    "            if results_mp.multi_face_landmarks:\n",
    "                try:\n",
    "                    landmarks = results_mp.multi_face_landmarks[0].landmark\n",
    "                    # ì™¼ìª½ ëˆˆ (386, 374), ì˜¤ë¥¸ìª½ ëˆˆ (159, 145)\n",
    "                    l_top = int(landmarks[386].y * ih)\n",
    "                    l_bottom = int(landmarks[374].y * ih)\n",
    "                    r_top = int(landmarks[159].y * ih)\n",
    "                    r_bottom = int(landmarks[145].y * ih)\n",
    "\n",
    "                    left_closed = is_eye_closed(l_top, l_bottom)\n",
    "                    right_closed = is_eye_closed(r_top, r_bottom)\n",
    "\n",
    "                    pred_label = 'closed' if (left_closed and right_closed) else 'open'\n",
    "\n",
    "                except Exception as e:\n",
    "                    pred_label = 'closed'  # ì—ëŸ¬ ë°œìƒ ì‹œ ê°ì€ ìƒíƒœë¡œ ê°„ì£¼\n",
    "\n",
    "            results.append({\n",
    "                'path': file_path,\n",
    "                'true_label': label,\n",
    "                'pred_label': pred_label\n",
    "            })\n",
    "\n",
    "# ì •í™•ë„ ê³„ì‚°\n",
    "total = len(results)\n",
    "correct = sum(1 for r in results if r['true_label'] == r['pred_label'])\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "\n",
    "print(\"\\nğŸ“Š ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½\")\n",
    "print(f\"ì´ ì´ë¯¸ì§€ ìˆ˜     : {total}\")\n",
    "print(f\"ì •í™•íˆ ì˜ˆì¸¡í•œ ìˆ˜ : {correct}\")\n",
    "print(f\"ì •í™•ë„           : {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c56ea63-68c8-496b-be39-2a234d898a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train/open] 500ì¥ ì²˜ë¦¬ ì¤‘...\n",
      "ëœë“œë§ˆí¬ ì¸ì‹ ì„±ê³µ ì´ë¯¸ì§€ ìˆ˜: 73/500\n",
      "[train/closed] 500ì¥ ì²˜ë¦¬ ì¤‘...\n",
      "ëœë“œë§ˆí¬ ì¸ì‹ ì„±ê³µ ì´ë¯¸ì§€ ìˆ˜: 86/500\n",
      "[val/open] 500ì¥ ì²˜ë¦¬ ì¤‘...\n",
      "ëœë“œë§ˆí¬ ì¸ì‹ ì„±ê³µ ì´ë¯¸ì§€ ìˆ˜: 172/500\n",
      "[val/closed] 500ì¥ ì²˜ë¦¬ ì¤‘...\n",
      "ëœë“œë§ˆí¬ ì¸ì‹ ì„±ê³µ ì´ë¯¸ì§€ ìˆ˜: 183/500\n",
      "[test/open] 500ì¥ ì²˜ë¦¬ ì¤‘...\n",
      "ëœë“œë§ˆí¬ ì¸ì‹ ì„±ê³µ ì´ë¯¸ì§€ ìˆ˜: 264/500\n",
      "[test/closed] 500ì¥ ì²˜ë¦¬ ì¤‘...\n",
      "ëœë“œë§ˆí¬ ì¸ì‹ ì„±ê³µ ì´ë¯¸ì§€ ìˆ˜: 271/500\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Mediapipe ì„¤ì •\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=True,\n",
    "    refine_landmarks=True,\n",
    "    max_num_faces=1,\n",
    "    min_detection_confidence=0.5\n",
    ")\n",
    "\n",
    "# ëˆˆ ê°ê¹€ íŒë‹¨ í•¨ìˆ˜ (ì™¼/ì˜¤ ìë™ ê°ì§€)\n",
    "def is_eye_closed_by_landmark(landmarks, image_height, threshold=3):\n",
    "    try:\n",
    "        l_top = landmarks[386].y * image_height\n",
    "        l_bottom = landmarks[374].y * image_height\n",
    "        if abs(l_top - l_bottom) < threshold:\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        r_top = landmarks[159].y * image_height\n",
    "        r_bottom = landmarks[145].y * image_height\n",
    "        if abs(r_top - r_bottom) < threshold:\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return True  # ë‘˜ ë‹¤ ê°ì§€ ì‹¤íŒ¨ ì‹œ ê°ì€ ê²ƒìœ¼ë¡œ ê°„ì£¼\n",
    "\n",
    "# ë°ì´í„°ì…‹ ê²½ë¡œ\n",
    "base_dir = r\"C:\\Users\\Users\\open-closed-eyes-dataset\"\n",
    "sets = ['train', 'val', 'test']\n",
    "labels = ['open', 'closed']\n",
    "max_images = 500\n",
    "landmark_detected = 0\n",
    "\n",
    "results = []\n",
    "\n",
    "for subset in sets:\n",
    "    for label in labels:\n",
    "        folder = os.path.join(base_dir, subset, label)\n",
    "        images = [f for f in os.listdir(folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        images = random.sample(images, min(max_images, len(images)))\n",
    "\n",
    "        print(f\"[{subset}/{label}] {len(images)}ì¥ ì²˜ë¦¬ ì¤‘...\")\n",
    "\n",
    "        for img_name in images:\n",
    "            path = os.path.join(folder, img_name)\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                continue\n",
    "\n",
    "            ih, iw, _ = img.shape\n",
    "            rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            result = face_mesh.process(rgb)\n",
    "\n",
    "            if result.multi_face_landmarks:\n",
    "                landmark_detected += 1\n",
    "\n",
    "            pred = \"closed\"  # ê¸°ë³¸ê°’ì€ ê°ì€ ìƒíƒœ\n",
    "\n",
    "            if result.multi_face_landmarks:\n",
    "                landmarks = result.multi_face_landmarks[0].landmark\n",
    "                closed = is_eye_closed_by_landmark(landmarks, ih)\n",
    "                pred = \"closed\" if closed else \"open\"\n",
    "\n",
    "            results.append({\n",
    "                'path': path,\n",
    "                'true_label': label,\n",
    "                'pred_label': pred\n",
    "            })\n",
    "        print(f\"ëœë“œë§ˆí¬ ì¸ì‹ ì„±ê³µ ì´ë¯¸ì§€ ìˆ˜: {landmark_detected}/{len(images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75db2f82-858c-4c04-82f9-b30e1401afce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ì´ ì´ë¯¸ì§€: 3000 / ì •í™•íˆ ë§ì¶˜ ìˆ˜: 1500 / ì •í™•ë„: 50.00%\n"
     ]
    }
   ],
   "source": [
    "total = len(results)\n",
    "correct = sum(1 for r in results if r['true_label'] == r['pred_label'])\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "\n",
    "print(f\"\\nğŸ“Š ì´ ì´ë¯¸ì§€: {total} / ì •í™•íˆ ë§ì¶˜ ìˆ˜: {correct} / ì •í™•ë„: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31f7263d-2be0-4c23-b46c-45485fc08456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜ˆì¸¡ ë¶„í¬: Counter({'closed': 3000})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "preds = [r['pred_label'] for r in results]\n",
    "counts = Counter(preds)\n",
    "print(\"ì˜ˆì¸¡ ë¶„í¬:\", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9551c699-cbc1-42f9-ab4c-d139f8a0c530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ìƒ˜í”Œ ìˆ˜: 408 / ê²€ì¦: 367 / í…ŒìŠ¤íŠ¸: 417\n",
      "âœ… í•™ìŠµ ì‹œì‘\n",
      "[Epoch 1] Loss: 4.7005 | Val Accuracy: 88.28%\n",
      "[Epoch 2] Loss: 4.3445 | Val Accuracy: 88.28%\n",
      "[Epoch 3] Loss: 4.2970 | Val Accuracy: 88.28%\n",
      "[Epoch 4] Loss: 4.2975 | Val Accuracy: 88.28%\n",
      "[Epoch 5] Loss: 4.3282 | Val Accuracy: 88.28%\n",
      "[Epoch 6] Loss: 4.3261 | Val Accuracy: 88.28%\n",
      "[Epoch 7] Loss: 4.3805 | Val Accuracy: 88.28%\n",
      "[Epoch 8] Loss: 4.2957 | Val Accuracy: 88.28%\n",
      "[Epoch 9] Loss: 4.2979 | Val Accuracy: 88.28%\n",
      "[Epoch 10] Loss: 4.3622 | Val Accuracy: 88.28%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "MAX_PER_CLASS = 2000\n",
    "\n",
    "# -------------------------\n",
    "# [1] Mediapipe landmark ì¶”ì¶œ\n",
    "# -------------------------\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, refine_landmarks=True)\n",
    "\n",
    "def extract_eye_landmarks(image):\n",
    "    ih, iw, _ = image.shape\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    result = face_mesh.process(rgb)\n",
    "\n",
    "    if not result.multi_face_landmarks:\n",
    "        return None  # ì–¼êµ´ ì¸ì‹ ì‹¤íŒ¨\n",
    "\n",
    "    landmarks = result.multi_face_landmarks[0].landmark\n",
    "    try:\n",
    "        left_top = landmarks[386].y * ih\n",
    "        left_bottom = landmarks[374].y * ih\n",
    "        right_top = landmarks[159].y * ih\n",
    "        right_bottom = landmarks[145].y * ih\n",
    "        return [left_top, left_bottom, right_top, right_bottom]\n",
    "    except:\n",
    "        return None  # ì¢Œí‘œ ì¶”ì¶œ ì‹¤íŒ¨\n",
    "\n",
    "# -------------------------\n",
    "# [2] ì´ë¯¸ì§€ ê²½ë¡œ ìˆœíšŒ ë° ì¢Œí‘œ ìˆ˜ì§‘\n",
    "# -------------------------\n",
    "def load_eye_data(base_dir, max_per_class=2000):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for label_name in ['open', 'closed']:\n",
    "        folder = os.path.join(base_dir, label_name)\n",
    "        files = [f for f in os.listdir(folder) if f.lower().endswith(('.jpg', '.png'))]\n",
    "        files = random.sample(files, min(max_per_class, len(files)))\n",
    "\n",
    "        label = 1 if label_name == 'open' else 0\n",
    "\n",
    "        for file in files:\n",
    "            path = os.path.join(folder, file)\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                continue\n",
    "\n",
    "            coords = extract_eye_landmarks(img)\n",
    "            if coords:\n",
    "                X.append(coords)\n",
    "                y.append(label)\n",
    "\n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.int64)\n",
    "\n",
    "# -------------------------\n",
    "# [3] PyTorch Dataset í´ë˜ìŠ¤\n",
    "# -------------------------\n",
    "class EyeLandmarkDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# -------------------------\n",
    "# [4] MLP ëª¨ë¸ ì •ì˜\n",
    "# -------------------------\n",
    "class EyeMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(4, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# -------------------------\n",
    "# [5] ë°ì´í„° ê²½ë¡œ ì„¤ì • (train/val/test í´ë” ê°ê° MAX_PER_CLASSê°œì”©)\n",
    "# -------------------------\n",
    "train_dir = r\"C:\\Users\\Users\\open-closed-eyes-dataset\\train\"\n",
    "val_dir = r\"C:\\Users\\Users\\open-closed-eyes-dataset\\val\"\n",
    "test_dir = r\"C:\\Users\\Users\\open-closed-eyes-dataset\\test\"\n",
    "\n",
    "X_train, y_train = load_eye_data(train_dir, max_per_class=MAX_PER_CLASS)\n",
    "X_val, y_val = load_eye_data(val_dir, max_per_class=MAX_PER_CLASS)\n",
    "X_test, y_test = load_eye_data(test_dir, max_per_class=MAX_PER_CLASS)\n",
    "\n",
    "print(f\"í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {len(X_train)} / ê²€ì¦: {len(X_val)} / í…ŒìŠ¤íŠ¸: {len(X_test)}\")\n",
    "\n",
    "# -------------------------\n",
    "# [6] Dataset/DataLoader\n",
    "# -------------------------\n",
    "train_loader = DataLoader(EyeLandmarkDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(EyeLandmarkDataset(X_val, y_val), batch_size=32)\n",
    "test_loader = DataLoader(EyeLandmarkDataset(X_test, y_test), batch_size=32)\n",
    "\n",
    "# -------------------------\n",
    "# [7] ëª¨ë¸ í•™ìŠµ\n",
    "# -------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = EyeMLP().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"âœ… í•™ìŠµ ì‹œì‘\")\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # ê²€ì¦\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            preds = model(X_batch).argmax(dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += len(y_batch)\n",
    "\n",
    "    val_acc = correct / total * 100\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {running_loss:.4f} | Val Accuracy: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1040e815-ae2c-4180-b2af-ebd1126e0e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í‰ê°€\n",
      "ğŸ¯ í…ŒìŠ¤íŠ¸ ì •í™•ë„: 96.95%\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# [8] í…ŒìŠ¤íŠ¸ í‰ê°€\n",
    "# -------------------------\n",
    "print(\"\\nâœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í‰ê°€\")\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        preds = model(X_batch).argmax(dim=1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total += len(y_batch)\n",
    "\n",
    "test_acc = correct / total * 100\n",
    "print(f\"ğŸ¯ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "112dffad-fd01-47d6-aca0-fbcb019a799c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# -----------------------\n",
    "# [1] í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "# -----------------------\n",
    "IMG_SIZE = 90\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------\n",
    "# [2] ì „ì²˜ë¦¬ ì •ì˜\n",
    "# -----------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# -----------------------\n",
    "# [3] ëœë¤ 500ê°œ ì¶”ì¶œ Dataset í´ë˜ìŠ¤\n",
    "# -----------------------\n",
    "class CustomEyeDataset(Dataset):\n",
    "    def __init__(self, root_dir, max_per_class=2000, transform=None):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        classes = ['open', 'closed']\n",
    "\n",
    "        for label, cls in enumerate(classes):\n",
    "            folder = os.path.join(root_dir, cls)\n",
    "            files = [f for f in os.listdir(folder) if f.lower().endswith(('.jpg', '.png'))]\n",
    "            random.shuffle(files)\n",
    "            files = files[:max_per_class]\n",
    "\n",
    "            for fname in files:\n",
    "                self.samples.append((os.path.join(folder, fname), label))\n",
    "\n",
    "        random.shuffle(self.samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# -----------------------\n",
    "# [4] ë°ì´í„°ì…‹ ë¡œë”\n",
    "# -----------------------\n",
    "train_dir = r\"C:\\Users\\Users\\open-closed-eyes-dataset\\train\"\n",
    "val_dir   = r\"C:\\Users\\Users\\open-closed-eyes-dataset\\val\"\n",
    "test_dir  = r\"C:\\Users\\Users\\open-closed-eyes-dataset\\test\"\n",
    "\n",
    "train_loader = DataLoader(CustomEyeDataset(train_dir, transform=transform), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(CustomEyeDataset(val_dir, transform=transform), batch_size=BATCH_SIZE)\n",
    "test_loader  = DataLoader(CustomEyeDataset(test_dir, transform=transform), batch_size=BATCH_SIZE)\n",
    "\n",
    "# -----------------------\n",
    "# [5] CNN ëª¨ë¸ ì •ì˜\n",
    "# -----------------------\n",
    "class EyeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 11 * 11, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = EyeCNN().to(device)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "62d99a3b-57cd-4625-ba34-f389ea62c36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ í…ŒìŠ¤íŠ¸ ì •í™•ë„: 96.95%\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 6. í…ŒìŠ¤íŠ¸ í‰ê°€\n",
    "# ---------------------------\n",
    "def evaluate(loader):\n",
    "    model.eval()  # í‰ê°€ ëª¨ë“œ ì „í™˜\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = model(X)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total * 100\n",
    "    \n",
    "test_acc = evaluate(test_loader)\n",
    "print(f\"\\nğŸ¯ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fa617ebd-c4ac-48d5-95d6-f0a9ca834ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ìƒ˜í”Œ ìˆ˜: 339 / ê²€ì¦: 325 / í…ŒìŠ¤íŠ¸: 288\n",
      "âœ… í•™ìŠµ ì‹œì‘\n",
      "[Epoch 1] Loss: 4.3233 | Val Accuracy: 92.92%\n",
      "[Epoch 2] Loss: 4.2116 | Val Accuracy: 92.92%\n",
      "[Epoch 3] Loss: 4.2671 | Val Accuracy: 92.92%\n",
      "[Epoch 4] Loss: 4.1870 | Val Accuracy: 92.92%\n",
      "[Epoch 5] Loss: 4.3676 | Val Accuracy: 92.92%\n",
      "[Epoch 6] Loss: 4.1704 | Val Accuracy: 92.92%\n",
      "[Epoch 7] Loss: 4.6892 | Val Accuracy: 92.92%\n",
      "[Epoch 8] Loss: 4.2520 | Val Accuracy: 92.92%\n",
      "[Epoch 9] Loss: 4.2774 | Val Accuracy: 92.92%\n",
      "[Epoch 10] Loss: 4.2715 | Val Accuracy: 92.92%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "MAX_PER_CLASS = 1666\n",
    "\n",
    "# -------------------------\n",
    "# [1] Mediapipe landmark ì¶”ì¶œ (í•œìª½ ëˆˆë§Œ)\n",
    "# -------------------------\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, refine_landmarks=True)\n",
    "\n",
    "def extract_one_eye_landmarks(image):\n",
    "    ih, iw, _ = image.shape\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    result = face_mesh.process(rgb)\n",
    "\n",
    "    if not result.multi_face_landmarks:\n",
    "        return None  # ì–¼êµ´ ì¸ì‹ ì‹¤íŒ¨\n",
    "\n",
    "    landmarks = result.multi_face_landmarks[0].landmark\n",
    "    try:\n",
    "        # ì˜¤ë¥¸ìª½ ëˆˆ ìœ„, ì•„ë˜ ì¢Œí‘œë§Œ ì‚¬ìš© (ì˜ˆ: 159(ìœ„), 145(ì•„ë˜))\n",
    "        top = landmarks[159].y * ih\n",
    "        bottom = landmarks[145].y * ih\n",
    "        return [top, bottom]\n",
    "    except:\n",
    "        return None  # ì¢Œí‘œ ì¶”ì¶œ ì‹¤íŒ¨\n",
    "\n",
    "# -------------------------\n",
    "# [2] ì´ë¯¸ì§€ ê²½ë¡œ ìˆœíšŒ ë° ì¢Œí‘œ ìˆ˜ì§‘\n",
    "# -------------------------\n",
    "def load_eye_data(base_dir, max_per_class=MAX_PER_CLASS):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for label_name in ['open', 'closed']:\n",
    "        folder = os.path.join(base_dir, label_name)\n",
    "        files = [f for f in os.listdir(folder) if f.lower().endswith(('.jpg', '.png'))]\n",
    "        files = random.sample(files, min(max_per_class, len(files)))\n",
    "\n",
    "        label = 1 if label_name == 'open' else 0\n",
    "\n",
    "        for file in files:\n",
    "            path = os.path.join(folder, file)\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                continue\n",
    "\n",
    "            coords = extract_one_eye_landmarks(img)\n",
    "            if coords:\n",
    "                X.append(coords)\n",
    "                y.append(label)\n",
    "\n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.int64)\n",
    "\n",
    "# -------------------------\n",
    "# [3] PyTorch Dataset í´ë˜ìŠ¤\n",
    "# -------------------------\n",
    "class EyeLandmarkDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# -------------------------\n",
    "# [4] MLP ëª¨ë¸ ì •ì˜ (ì…ë ¥ ë…¸ë“œ 2ê°œë¡œ ë³€ê²½)\n",
    "# -------------------------\n",
    "class EyeMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 32),   # ì…ë ¥ ë…¸ë“œ 2ê°œ -> ì€ë‹‰ì¸µ 32ê°œ ë…¸ë“œ\n",
    "            nn.ReLU(),          # ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ ReLU\n",
    "            nn.Linear(32, 16),  # ì€ë‹‰ì¸µ 32ê°œ -> ì€ë‹‰ì¸µ 16ê°œ\n",
    "            nn.ReLU(),          # ReLU í™œì„±í™”\n",
    "            nn.Linear(16, 2)    # ì€ë‹‰ì¸µ 16ê°œ -> ì¶œë ¥ì¸µ 2ê°œ (open/closed ë¶„ë¥˜)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# -------------------------\n",
    "# [5] ë°ì´í„° ê²½ë¡œ ì„¤ì • ë° ë°ì´í„° ë¡œë”©\n",
    "# -------------------------\n",
    "train_dir = r\"C:\\Users\\Users\\open-closed-eyes-dataset\\train\"\n",
    "val_dir = r\"C:\\Users\\Users\\open-closed-eyes-dataset\\val\"\n",
    "test_dir = r\"C:\\Users\\Users\\open-closed-eyes-dataset\\test\"\n",
    "\n",
    "X_train, y_train = load_eye_data(train_dir, max_per_class=MAX_PER_CLASS)\n",
    "X_val, y_val = load_eye_data(val_dir, max_per_class=MAX_PER_CLASS)\n",
    "X_test, y_test = load_eye_data(test_dir, max_per_class=MAX_PER_CLASS)\n",
    "\n",
    "print(f\"í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {len(X_train)} / ê²€ì¦: {len(X_val)} / í…ŒìŠ¤íŠ¸: {len(X_test)}\")\n",
    "\n",
    "# -------------------------\n",
    "# [6] Dataset/DataLoader ìƒì„±\n",
    "# -------------------------\n",
    "train_loader = DataLoader(EyeLandmarkDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(EyeLandmarkDataset(X_val, y_val), batch_size=32)\n",
    "test_loader = DataLoader(EyeLandmarkDataset(X_test, y_test), batch_size=32)\n",
    "\n",
    "# -------------------------\n",
    "# [7] ëª¨ë¸ í•™ìŠµ\n",
    "# -------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = EyeMLP().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"âœ… í•™ìŠµ ì‹œì‘\")\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            preds = model(X_batch).argmax(dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += len(y_batch)\n",
    "\n",
    "    val_acc = correct / total * 100\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {running_loss:.4f} | Val Accuracy: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "08a364d4-6344-4770-a951-51311d31786a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í‰ê°€\n",
      "ğŸ¯ í…ŒìŠ¤íŠ¸ ì •í™•ë„: 89.58%\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# [8] í…ŒìŠ¤íŠ¸ í‰ê°€\n",
    "# -------------------------\n",
    "print(\"\\nâœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í‰ê°€\")\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        preds = model(X_batch).argmax(dim=1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total += len(y_batch)\n",
    "\n",
    "test_acc = correct / total * 100\n",
    "print(f\"ğŸ¯ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3d22ac02-30df-4378-b81d-78fec924f986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ë¯¸ì§€ íŒŒì¼ ê°œìˆ˜: 6665\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# test - closed 1666ê°œ\n",
    "# test - open 5325ê°œ\n",
    "# train - closed 33322ê°œ\n",
    "# train - open 106482ê°œ\n",
    "# val - closed 6665 ê°œ\n",
    "# val - open 21296 ê°œ\n",
    "folder_path = r\"C:\\Users\\Users\\open-closed-eyes-dataset\\val\\closed\"\n",
    "\n",
    "# ì´ë¯¸ì§€ í™•ì¥ì ë¦¬ìŠ¤íŠ¸\n",
    "image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.gif')\n",
    "\n",
    "files = [f for f in os.listdir(folder_path) if f.lower().endswith(image_extensions)]\n",
    "print(f\"ì´ë¯¸ì§€ íŒŒì¼ ê°œìˆ˜: {len(files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3c681bc0-d48b-47e9-910d-966f96088881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ìƒ˜í”Œ ìˆ˜: 3332 / ê²€ì¦: 3332 / í…ŒìŠ¤íŠ¸: 3332\n",
      "Train Accuracy: 50.00%\n",
      "Validation Accuracy: 50.00%\n",
      "Test Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "MAX_PER_CLASS = 1666\n",
    "\n",
    "# -------------------------\n",
    "# [1] ëˆˆ ì˜ì—­ì—ì„œ ìœ„, ì•„ë˜ y ì¢Œí‘œ ì¶”ì¶œ (ì–¼êµ´ ì¸ì‹ ì œê±°)\n",
    "# -------------------------\n",
    "def extract_one_eye_landmarks_direct(image):\n",
    "    ih, iw, _ = image.shape\n",
    "\n",
    "    # ëˆˆ ì˜ì—­ë§Œ ìˆë‹¤ê³  ê°€ì •, ì´ë¯¸ì§€ ë†’ì´ë¥¼ 3ë“±ë¶„í•˜ì—¬ ìœ„, ì•„ë˜ ì¢Œí‘œ ì¶”ì¶œ ì˜ˆì‹œ\n",
    "    top = ih * 1/3\n",
    "    bottom = ih * 2/3\n",
    "    return [top, bottom]\n",
    "\n",
    "# -------------------------\n",
    "# [2] ì´ë¯¸ì§€ ê²½ë¡œ ìˆœíšŒ ë° ì¢Œí‘œ ìˆ˜ì§‘\n",
    "# -------------------------\n",
    "def load_eye_data(base_dir, max_per_class=MAX_PER_CLASS):\n",
    "    data = []\n",
    "\n",
    "    for label_name in ['open', 'closed']:\n",
    "        folder = os.path.join(base_dir, label_name)\n",
    "        files = [f for f in os.listdir(folder) if f.lower().endswith(('.jpg', '.png'))]\n",
    "        files = random.sample(files, min(max_per_class, len(files)))\n",
    "\n",
    "        label = 1 if label_name == 'open' else 0\n",
    "\n",
    "        for file in files:\n",
    "            path = os.path.join(folder, file)\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                continue\n",
    "\n",
    "            coords = extract_one_eye_landmarks_direct(img)\n",
    "            if coords:\n",
    "                top, bottom = coords\n",
    "                distance = bottom - top\n",
    "                data.append((distance, label))\n",
    "\n",
    "    return data\n",
    "\n",
    "# -------------------------\n",
    "# [3] ê°„ë‹¨í•œ ì„ê³„ê°’ ê¸°ë°˜ ë¶„ë¥˜\n",
    "# -------------------------\n",
    "def simple_threshold_classifier(data, threshold):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for distance, label in data:\n",
    "        pred = 0 if distance < threshold else 1  # ê±°ë¦¬ ì‘ìœ¼ë©´ ê°ê¹€(closed=0), í¬ë©´ ëœ¸(open=1)\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    return accuracy\n",
    "\n",
    "# -------------------------\n",
    "# [4] ë°ì´í„° ê²½ë¡œ ì„¤ì • ë° ë°ì´í„° ë¡œë”©\n",
    "# -------------------------\n",
    "train_dir = r\"C:\\Users\\Users\\open-closed-eyes-dataset\\train\"\n",
    "val_dir = r\"C:\\Users\\Users\\open-closed-eyes-dataset\\val\"\n",
    "test_dir = r\"C:\\Users\\Users\\open-closed-eyes-dataset\\test\"\n",
    "\n",
    "train_data = load_eye_data(train_dir)\n",
    "val_data = load_eye_data(val_dir)\n",
    "test_data = load_eye_data(test_dir)\n",
    "\n",
    "print(f\"í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {len(train_data)} / ê²€ì¦: {len(val_data)} / í…ŒìŠ¤íŠ¸: {len(test_data)}\")\n",
    "\n",
    "# -------------------------\n",
    "# [5] ì„ê³„ê°’ ì„¤ì • (ì˜ˆ: ê²½í—˜ì ìœ¼ë¡œ ì •í•˜ê±°ë‚˜ ê²€ì¦ ë°ì´í„°ì—ì„œ íƒìƒ‰)\n",
    "# -------------------------\n",
    "threshold = 50  # í”½ì…€ ë‹¨ìœ„ ì„ì˜ ê°’, ì ì ˆíˆ ì¡°ì • í•„ìš”\n",
    "\n",
    "# -------------------------\n",
    "# [6] ì •í™•ë„ í‰ê°€\n",
    "# -------------------------\n",
    "train_acc = simple_threshold_classifier(train_data, threshold)\n",
    "val_acc = simple_threshold_classifier(val_data, threshold)\n",
    "test_acc = simple_threshold_classifier(test_data, threshold)\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc:.2f}%\")\n",
    "print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7d51e898-7c3c-4d3d-8131-fe066e97812d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ìƒ˜í”Œ ìˆ˜: 2766 / ê²€ì¦: 2714 / í…ŒìŠ¤íŠ¸: 2719\n",
      "Train Accuracy: 52.31%\n",
      "Validation Accuracy: 53.35%\n",
      "Test Accuracy: 53.51%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "MAX_PER_CLASS = 1666\n",
    "\n",
    "# -------------------------\n",
    "# [1] ëˆˆ ì´ë¯¸ì§€ì—ì„œ ëˆˆêº¼í’€ ìœ„Â·ì•„ë˜ y ì¢Œí‘œ ì§ì ‘ ì¶”ì¶œ\n",
    "# -------------------------\n",
    "def extract_eye_top_bottom(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # ëˆˆë™ìëŠ” ë³´í†µ ì–´ë‘ì›Œì„œ ì—­ì´ì§„í™” (ì–´ë‘ìš´ ì˜ì—­ì„ í°ìƒ‰ìœ¼ë¡œ)\n",
    "    _, thresh = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # ìˆ˜í‰ í”½ì…€ í•© (yì¶• ë°©í–¥)\n",
    "    horizontal_sum = np.sum(thresh, axis=1)\n",
    "\n",
    "    ys = np.where(horizontal_sum > 0)[0]\n",
    "\n",
    "    if len(ys) == 0:\n",
    "        return None\n",
    "\n",
    "    top = ys[0]\n",
    "    bottom = ys[-1]\n",
    "\n",
    "    return top, bottom\n",
    "\n",
    "# -------------------------\n",
    "# [2] ì´ë¯¸ì§€ í´ë” ìˆœíšŒí•˜ë©° ë°ì´í„°(ëˆˆêº¼í’€ ê±°ë¦¬, ë¼ë²¨) ìˆ˜ì§‘\n",
    "# -------------------------\n",
    "def load_eye_data(base_dir, max_per_class=MAX_PER_CLASS):\n",
    "    data = []\n",
    "\n",
    "    for label_name in ['open', 'closed']:\n",
    "        folder = os.path.join(base_dir, label_name)\n",
    "        files = [f for f in os.listdir(folder) if f.lower().endswith(('.jpg', '.png'))]\n",
    "        files = random.sample(files, min(max_per_class, len(files)))\n",
    "\n",
    "        label = 1 if label_name == 'open' else 0\n",
    "\n",
    "        for file in files:\n",
    "            path = os.path.join(folder, file)\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                continue\n",
    "\n",
    "            coords = extract_eye_top_bottom(img)\n",
    "            if coords:\n",
    "                top, bottom = coords\n",
    "                distance = bottom - top\n",
    "                data.append((distance, label))\n",
    "\n",
    "    return data\n",
    "\n",
    "# -------------------------\n",
    "# [3] ì„ê³„ê°’ ê¸°ë°˜ ëˆˆ ê°ê¹€ ë¶„ë¥˜ê¸°\n",
    "# -------------------------\n",
    "def simple_threshold_classifier(data, threshold):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for distance, label in data:\n",
    "        pred = 0 if distance < threshold else 1\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    accuracy = correct / total * 100 if total > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "# -------------------------\n",
    "# [4] ë°ì´í„° ê²½ë¡œ ì„¤ì • ë° ë°ì´í„° ë¡œë”©\n",
    "# -------------------------\n",
    "train_dir = r\"C:\\Users\\Users\\open-closed-eyes-dataset\\train\"\n",
    "val_dir = r\"C:\\Users\\Users\\open-closed-eyes-dataset\\val\"\n",
    "test_dir = r\"C:\\Users\\Users\\open-closed-eyes-dataset\\test\"\n",
    "\n",
    "train_data = load_eye_data(train_dir)\n",
    "val_data = load_eye_data(val_dir)\n",
    "test_data = load_eye_data(test_dir)\n",
    "\n",
    "print(f\"í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {len(train_data)} / ê²€ì¦: {len(val_data)} / í…ŒìŠ¤íŠ¸: {len(test_data)}\")\n",
    "\n",
    "# -------------------------\n",
    "# [5] ê²½í—˜ì ìœ¼ë¡œ ì •í•œ ì„ê³„ê°’ (í”½ì…€ ë‹¨ìœ„, ëˆˆ ì´ë¯¸ì§€ í¬ê¸°ì— ë”°ë¼ ì¡°ì • í•„ìš”)\n",
    "# -------------------------\n",
    "threshold = 15\n",
    "\n",
    "# -------------------------\n",
    "# [6] ì •í™•ë„ í‰ê°€\n",
    "# -------------------------\n",
    "train_acc = simple_threshold_classifier(train_data, threshold)\n",
    "val_acc = simple_threshold_classifier(val_data, threshold)\n",
    "test_acc = simple_threshold_classifier(test_data, threshold)\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc:.2f}%\")\n",
    "print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676d21a1-c22d-47b3-b7fe-ce573239638b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
