{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9340a4bf-9494-465f-a8e4-1e842c76dc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í•™ìŠµ ì‹œì‘\n",
      "ğŸ“Š Train í´ë˜ìŠ¤ ë¶„í¬: ['closed', 'open']\n",
      "Train ìƒ˜í”Œ ìˆ˜: 66644\n",
      "Val ìƒ˜í”Œ ìˆ˜: 13330\n",
      "Test ìƒ˜í”Œ ìˆ˜: 3332\n",
      "[Epoch 1] Loss: 294.4559 | Val Accuracy: 92.01%\n",
      "[Epoch 2] Loss: 193.2394 | Val Accuracy: 93.74%\n",
      "[Epoch 3] Loss: 155.0744 | Val Accuracy: 95.06%\n",
      "[Epoch 4] Loss: 128.8231 | Val Accuracy: 95.35%\n",
      "[Epoch 5] Loss: 109.0905 | Val Accuracy: 96.20%\n",
      "[Epoch 6] Loss: 97.1555 | Val Accuracy: 96.43%\n",
      "[Epoch 7] Loss: 85.9808 | Val Accuracy: 96.77%\n",
      "[Epoch 8] Loss: 77.8967 | Val Accuracy: 96.86%\n",
      "[Epoch 9] Loss: 71.7292 | Val Accuracy: 97.09%\n",
      "[Epoch 10] Loss: 66.4070 | Val Accuracy: 97.37%\n",
      "ğŸ§ª í…ŒìŠ¤íŠ¸ ì •í™•ë„: 97.54%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# -------------------------\n",
    "# [1] ì„¤ì •\n",
    "# -------------------------\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LR = 0.00005\n",
    "IMG_SIZE = 90\n",
    "MAX_PER_CLASS_test = 1666  # í´ë˜ìŠ¤ë‹¹ ìƒ˜í”Œ ìˆ˜\n",
    "MAX_PER_CLASS_val = 6665\n",
    "MAX_PER_CLASS_train = 33322\n",
    "\n",
    "train_dir = r\"C:\\Users\\Users\\open-closed-eyes-dataset\\train\"\n",
    "val_dir   = r\"C:\\Users\\Users\\open-closed-eyes-dataset\\val\"\n",
    "test_dir  = r\"C:\\Users\\Users\\open-closed-eyes-dataset\\test\"\n",
    "\n",
    "# -------------------------\n",
    "# [2] ì „ì²˜ë¦¬ ì •ì˜\n",
    "# -------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# -------------------------\n",
    "# [3] í´ë˜ìŠ¤ë³„ ê· í˜• ìƒ˜í”Œë§ í•¨ìˆ˜\n",
    "# -------------------------\n",
    "def balanced_subset(dataset, max_per_class):\n",
    "    targets = np.array(dataset.targets)\n",
    "    indices = []\n",
    "\n",
    "    for class_idx in range(len(dataset.classes)):\n",
    "        class_indices = np.where(targets == class_idx)[0]\n",
    "        sampled = random.sample(list(class_indices), min(len(class_indices), max_per_class))\n",
    "        indices.extend(sampled)\n",
    "\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "# -------------------------\n",
    "# [4] ë°ì´í„°ì…‹ ë° ë¡œë” ìƒì„±\n",
    "# -------------------------\n",
    "train_dataset = balanced_subset(ImageFolder(train_dir, transform=transform), MAX_PER_CLASS_train)\n",
    "val_dataset   = balanced_subset(ImageFolder(val_dir, transform=transform), MAX_PER_CLASS_val)\n",
    "test_dataset  = balanced_subset(ImageFolder(test_dir, transform=transform), MAX_PER_CLASS_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# -------------------------\n",
    "# [5] CNN ëª¨ë¸ ì •ì˜\n",
    "# -------------------------\n",
    "class EyeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1),  # 90x90 -> 90x90\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                 # 90x90 -> 45x45\n",
    "\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                 # 45x45 -> 22x22\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                 # 22x22 -> 11x11\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),  # ì¶”ê°€\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                   # ë” ì‘ì€ íŠ¹ì„± ë§µ\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 5 * 5, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2)  # 2 classes: open/closed\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# -------------------------\n",
    "# [6] í•™ìŠµ ì„¤ì •\n",
    "# -------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = EyeCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# -------------------------\n",
    "# [7] í•™ìŠµ ë£¨í”„\n",
    "# -------------------------\n",
    "print(\"âœ… í•™ìŠµ ì‹œì‘\")\n",
    "print(f\"ğŸ“Š Train í´ë˜ìŠ¤ ë¶„í¬: {[train_dataset.dataset.classes[i] for i in np.unique(train_dataset.dataset.targets)]}\")\n",
    "print(f\"Train ìƒ˜í”Œ ìˆ˜: {len(train_dataset)}\")\n",
    "print(f\"Val ìƒ˜í”Œ ìˆ˜: {len(val_dataset)}\")\n",
    "print(f\"Test ìƒ˜í”Œ ìˆ˜: {len(test_dataset)}\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # ê²€ì¦ ì •í™•ë„\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            preds = model(imgs).argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += len(labels)\n",
    "\n",
    "    val_acc = correct / total * 100\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {running_loss:.4f} | Val Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "# -------------------------\n",
    "# [8] í…ŒìŠ¤íŠ¸ ì •í™•ë„\n",
    "# -------------------------\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        preds = model(imgs).argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "test_acc = correct / total * 100\n",
    "print(f\"ğŸ§ª í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cd7c80-fb5d-43ae-9218-ca11864ec433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
